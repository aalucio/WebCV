{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Instalación y pruebas Hadoop y HDFS\n",
        "\n",
        "Alumno: Alvaro Lucio-Villegas de Cea"
      ],
      "metadata": {
        "id": "pPKEQNWMx-bZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instalación de Hadoop\n",
        "\n",
        "Hadoop es un marco de programación basado en Java que permite procesar y almacenar conjuntos de datos extremadamente grandes en un clúster de máquinas de bajo coste. Fue el primer gran proyecto de código abierto en el ámbito del Big Data y está patrocinado por la Apache Software Foundation."
      ],
      "metadata": {
        "id": "TTvVoompp8-D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Paso 1: Instalación de Haddoop"
      ],
      "metadata": {
        "id": "U7Cr4W9PqY1x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "U26D-ATvp8Jt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "242d1fe3-8714-4ef1-8d48-d57fd38ba4e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-02-22 18:13:08--  https://archive.apache.org/dist/hadoop/common/hadoop-3.3.4/hadoop-3.3.4.tar.gz\n",
            "Resolving archive.apache.org (archive.apache.org)... 138.201.131.134, 2a01:4f8:172:2ec5::2\n",
            "Connecting to archive.apache.org (archive.apache.org)|138.201.131.134|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 695457782 (663M) [application/x-gzip]\n",
            "Saving to: ‘hadoop-3.3.4.tar.gz’\n",
            "\n",
            "hadoop-3.3.4.tar.gz 100%[===================>] 663.24M  6.40MB/s    in 75s     \n",
            "\n",
            "2023-02-22 18:14:23 (8.89 MB/s) - ‘hadoop-3.3.4.tar.gz’ saved [695457782/695457782]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://archive.apache.org/dist/hadoop/common/hadoop-3.3.4/hadoop-3.3.4.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utilizaremos el comando tar con los parámetros:\n",
        "\n",
        "\n",
        "-x para extraer,\n",
        "\n",
        "-z para descomprimir,\n",
        "\n",
        "-f para especificar que estamos extrayendo de un archivo\n",
        "\n",
        "(podremos añadir -v para la salida en detalle)"
      ],
      "metadata": {
        "id": "T74g5GHjs3Yw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xzf hadoop-3.3.4.tar.gz"
      ],
      "metadata": {
        "id": "wSkE8_HwtDW8"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Copiamos el directorio de hadoop en /usr/local"
      ],
      "metadata": {
        "id": "aVW9Vy-2tQ6T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r hadoop-3.3.4 /usr/local/"
      ],
      "metadata": {
        "id": "hLiYlo_CtbxW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Paso 2: Configurar el Hadoop JAVA HOME\n",
        "Hadoop requiere que se establezca la ruta de acceso a Java, ya sea como una variable de entorno o en el archivo de configuración de Hadoop."
      ],
      "metadata": {
        "id": "etL3_KXZtol8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Buscamos cual es la dirección de Java en la máquina Google Colab\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7-XQYmoFtvHd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!readlink -f /usr/bin/java | sed \"s:bin/java::\""
      ],
      "metadata": {
        "id": "N1i-objMt7_i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fcc8afb-c1d7-4a7a-9939-bad0df916da4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/lib/jvm/java-11-openjdk-amd64/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.   Establecemos mediante código Python el valor de esta variable\n",
        "\n"
      ],
      "metadata": {
        "id": "eGbpJfYNuHVp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64/\""
      ],
      "metadata": {
        "id": "5XTXjmfcuTR-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Paso 3: Ejecutando Hadoop"
      ],
      "metadata": {
        "id": "EFT_a4toufMD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comprobamos la versión de Hadoop:"
      ],
      "metadata": {
        "id": "-N3opWUDuxa9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!/usr/local/hadoop-3.3.4/bin/hadoop version"
      ],
      "metadata": {
        "id": "cHOX1ZHOuzCR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39edd679-e72f-4cd9-cd10-84dd1aa9d51f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hadoop 3.3.4\n",
            "Source code repository https://github.com/apache/hadoop.git -r a585a73c3e02ac62350c136643a5e7f6095a3dbb\n",
            "Compiled by stevel on 2022-07-29T12:32Z\n",
            "Compiled with protoc 3.7.1\n",
            "From source with checksum fb9dd8918a7b8a5b430d61af858f6ec\n",
            "This command was run using /usr/local/hadoop-3.3.4/share/hadoop/common/hadoop-common-3.3.4.jar\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ejecutamos el comando sin parámetros:"
      ],
      "metadata": {
        "id": "MkePyDe9u52N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!/usr/local/hadoop-3.3.4/bin/hadoop"
      ],
      "metadata": {
        "id": "q7qd8O3Du7iK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37f70a78-7b5d-4472-ebba-ba6c64b0f818"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Usage: hadoop [OPTIONS] SUBCOMMAND [SUBCOMMAND OPTIONS]\n",
            " or    hadoop [OPTIONS] CLASSNAME [CLASSNAME OPTIONS]\n",
            "  where CLASSNAME is a user-provided Java class\n",
            "\n",
            "  OPTIONS is none or any of:\n",
            "\n",
            "buildpaths                       attempt to add class files from build tree\n",
            "--config dir                     Hadoop config directory\n",
            "--debug                          turn on shell script debug mode\n",
            "--help                           usage information\n",
            "hostnames list[,of,host,names]   hosts to use in slave mode\n",
            "hosts filename                   list of hosts to use in slave mode\n",
            "loglevel level                   set the log4j level for this command\n",
            "workers                          turn on worker mode\n",
            "\n",
            "  SUBCOMMAND is one of:\n",
            "\n",
            "\n",
            "    Admin Commands:\n",
            "\n",
            "daemonlog     get/set the log level for each daemon\n",
            "\n",
            "    Client Commands:\n",
            "\n",
            "archive       create a Hadoop archive\n",
            "checknative   check native Hadoop and compression libraries availability\n",
            "classpath     prints the class path needed to get the Hadoop jar and the\n",
            "              required libraries\n",
            "conftest      validate configuration XML files\n",
            "credential    interact with credential providers\n",
            "distch        distributed metadata changer\n",
            "distcp        copy file or directories recursively\n",
            "dtutil        operations related to delegation tokens\n",
            "envvars       display computed Hadoop environment variables\n",
            "fs            run a generic filesystem user client\n",
            "gridmix       submit a mix of synthetic job, modeling a profiled from\n",
            "              production load\n",
            "jar <jar>     run a jar file. NOTE: please use \"yarn jar\" to launch YARN\n",
            "              applications, not this command.\n",
            "jnipath       prints the java.library.path\n",
            "kdiag         Diagnose Kerberos Problems\n",
            "kerbname      show auth_to_local principal conversion\n",
            "key           manage keys via the KeyProvider\n",
            "rumenfolder   scale a rumen input trace\n",
            "rumentrace    convert logs into a rumen trace\n",
            "s3guard       manage metadata on S3\n",
            "trace         view and modify Hadoop tracing settings\n",
            "version       print the version\n",
            "\n",
            "    Daemon Commands:\n",
            "\n",
            "kms           run KMS, the Key Management Server\n",
            "registrydns   run the registry DNS server\n",
            "\n",
            "SUBCOMMAND may print help when invoked w/o parameters or with -h.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Una de las formas tradicionales de asegurarnos que un ambiente de Hadoop recién instalado funciona correctamente, es ejecutar el jar de ejemplos map-reduce incluido con todas las distribucioines de hadoop (hadoop-mapreduce-examples.jar)."
      ],
      "metadata": {
        "id": "phr6wo-EvGUp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Creamos un directorio llamado input en nuestro directorio de inicio"
      ],
      "metadata": {
        "id": "RpXKshVyvT_c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir ~/input"
      ],
      "metadata": {
        "id": "2mPXXbQwvM4c"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.   Copiamos los archivos de configuración (los xml) de Hadoop para usar esos archivos como nuestros datos de entrada."
      ],
      "metadata": {
        "id": "z3gRhT8wvgDC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /usr/local/hadoop-3.3.4/etc/hadoop/*.xml ~/input\n",
        "!ls ~/input"
      ],
      "metadata": {
        "id": "-_jS_Aq8vr8a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e924a1e-7569-4078-804a-807e9a205d2f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "capacity-scheduler.xml\thdfs-rbf-site.xml  kms-acls.xml     yarn-site.xml\n",
            "core-site.xml\t\thdfs-site.xml\t   kms-site.xml\n",
            "hadoop-policy.xml\thttpfs-site.xml    mapred-site.xml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.   Ejecutamos hadoop jar con el fin de ejecutar uno de los ejemplos por defecto, en este caso el grep que busca expresiones regulares dentro de los ficheros que le especifiquemos."
      ],
      "metadata": {
        "id": "tBdfkHoTvx4Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   /usr/local/hadoop/bin/hadoop Es el directorio donde esta el ejecutable de hadoop en el sistema.\n",
        "*   jar Le indica a hadoop que deseamos ejecutar una aplicacion empaquetada de Java. (Jar)\n",
        "*   /usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.4.jar Es la ruta donde esta el Jar que deseamos ejecutar. La versión del jar depende de la versión de hadoop instalada.\n",
        "*   grep Es un parámetro de los muchos que se le pueden pasar al Jar de ejemplos que trae Hadoop. grep sirve para encontrar y contar ocurrencias de strings haciendo uso de expresiones regulares.\n",
        "*   ~/input El directorio de entrada. Es donde el programa va a buscar los archivos de entrada a la tarea de map-reduce. Aquí copiamos unos archivos de prueba en un comando anterior.\n",
        "*   ~/grep_example El directorio de salida. Es donde el programa va a escribir el resultado de la corrida de la aplicación. En este caso, la cantidad de veces que la palabra del parámetro siguiente, aparece en los archivos de entrada.\n",
        "*   ‘allowed[.]*’ Es la expresión regular que deseamos buscar."
      ],
      "metadata": {
        "id": "eo35loL7v-Zz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "/usr/local/hadoop-3.3.4/bin/hadoop jar \\\n",
        "  /usr/local/hadoop-3.3.4/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.4.jar \\\n",
        "  grep ~/input ~/grep_example 'allowed[.]*'"
      ],
      "metadata": {
        "id": "HHcNVQyPv63K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "029e966e-4434-450e-c294-23ac923e39f3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-02-22 18:14:49,140 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2023-02-22 18:14:49,471 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2023-02-22 18:14:49,471 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2023-02-22 18:14:49,758 INFO input.FileInputFormat: Total input files to process : 10\n",
            "2023-02-22 18:14:49,792 INFO mapreduce.JobSubmitter: number of splits:10\n",
            "2023-02-22 18:14:50,202 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1421357974_0001\n",
            "2023-02-22 18:14:50,202 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2023-02-22 18:14:50,467 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2023-02-22 18:14:50,468 INFO mapreduce.Job: Running job: job_local1421357974_0001\n",
            "2023-02-22 18:14:50,478 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2023-02-22 18:14:50,492 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2023-02-22 18:14:50,492 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2023-02-22 18:14:50,493 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
            "2023-02-22 18:14:50,584 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2023-02-22 18:14:50,590 INFO mapred.LocalJobRunner: Starting task: attempt_local1421357974_0001_m_000000_0\n",
            "2023-02-22 18:14:50,642 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2023-02-22 18:14:50,645 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2023-02-22 18:14:50,685 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2023-02-22 18:14:50,695 INFO mapred.MapTask: Processing split: file:/root/input/hadoop-policy.xml:0+11765\n",
            "2023-02-22 18:14:50,838 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2023-02-22 18:14:50,838 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2023-02-22 18:14:50,838 INFO mapred.MapTask: soft limit at 83886080\n",
            "2023-02-22 18:14:50,838 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2023-02-22 18:14:50,838 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2023-02-22 18:14:50,871 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2023-02-22 18:14:50,896 INFO mapred.LocalJobRunner: \n",
            "2023-02-22 18:14:50,896 INFO mapred.MapTask: Starting flush of map output\n",
            "2023-02-22 18:14:50,896 INFO mapred.MapTask: Spilling map output\n",
            "2023-02-22 18:14:50,896 INFO mapred.MapTask: bufstart = 0; bufend = 374; bufvoid = 104857600\n",
            "2023-02-22 18:14:50,896 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214312(104857248); length = 85/6553600\n",
            "2023-02-22 18:14:50,919 INFO mapred.MapTask: Finished spill 0\n",
            "2023-02-22 18:14:50,934 INFO mapred.Task: Task:attempt_local1421357974_0001_m_000000_0 is done. And is in the process of committing\n",
            "2023-02-22 18:14:50,938 INFO mapred.LocalJobRunner: map\n",
            "2023-02-22 18:14:50,938 INFO mapred.Task: Task 'attempt_local1421357974_0001_m_000000_0' done.\n",
            "2023-02-22 18:14:50,948 INFO mapred.Task: Final Counters for attempt_local1421357974_0001_m_000000_0: Counters: 18\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=293934\n",
            "\t\tFILE: Number of bytes written=920525\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=275\n",
            "\t\tMap output records=22\n",
            "\t\tMap output bytes=374\n",
            "\t\tMap output materialized bytes=25\n",
            "\t\tInput split bytes=99\n",
            "\t\tCombine input records=22\n",
            "\t\tCombine output records=1\n",
            "\t\tSpilled Records=1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=23\n",
            "\t\tTotal committed heap usage (bytes)=357564416\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=11765\n",
            "2023-02-22 18:14:50,949 INFO mapred.LocalJobRunner: Finishing task: attempt_local1421357974_0001_m_000000_0\n",
            "2023-02-22 18:14:50,950 INFO mapred.LocalJobRunner: Starting task: attempt_local1421357974_0001_m_000001_0\n",
            "2023-02-22 18:14:50,951 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2023-02-22 18:14:50,951 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2023-02-22 18:14:50,952 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2023-02-22 18:14:50,954 INFO mapred.MapTask: Processing split: file:/root/input/capacity-scheduler.xml:0+9213\n",
            "2023-02-22 18:14:51,018 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2023-02-22 18:14:51,020 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2023-02-22 18:14:51,020 INFO mapred.MapTask: soft limit at 83886080\n",
            "2023-02-22 18:14:51,020 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2023-02-22 18:14:51,020 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2023-02-22 18:14:51,029 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2023-02-22 18:14:51,049 INFO mapred.LocalJobRunner: \n",
            "2023-02-22 18:14:51,049 INFO mapred.MapTask: Starting flush of map output\n",
            "2023-02-22 18:14:51,051 INFO mapred.MapTask: Spilling map output\n",
            "2023-02-22 18:14:51,051 INFO mapred.MapTask: bufstart = 0; bufend = 16; bufvoid = 104857600\n",
            "2023-02-22 18:14:51,051 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214396(104857584); length = 1/6553600\n",
            "2023-02-22 18:14:51,056 INFO mapred.MapTask: Finished spill 0\n",
            "2023-02-22 18:14:51,064 INFO mapred.Task: Task:attempt_local1421357974_0001_m_000001_0 is done. And is in the process of committing\n",
            "2023-02-22 18:14:51,066 INFO mapred.LocalJobRunner: map\n",
            "2023-02-22 18:14:51,067 INFO mapred.Task: Task 'attempt_local1421357974_0001_m_000001_0' done.\n",
            "2023-02-22 18:14:51,070 INFO mapred.Task: Final Counters for attempt_local1421357974_0001_m_000001_0: Counters: 18\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=304139\n",
            "\t\tFILE: Number of bytes written=920581\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=244\n",
            "\t\tMap output records=1\n",
            "\t\tMap output bytes=16\n",
            "\t\tMap output materialized bytes=24\n",
            "\t\tInput split bytes=104\n",
            "\t\tCombine input records=1\n",
            "\t\tCombine output records=1\n",
            "\t\tSpilled Records=1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=36\n",
            "\t\tTotal committed heap usage (bytes)=357564416\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=9213\n",
            "2023-02-22 18:14:51,071 INFO mapred.LocalJobRunner: Finishing task: attempt_local1421357974_0001_m_000001_0\n",
            "2023-02-22 18:14:51,071 INFO mapred.LocalJobRunner: Starting task: attempt_local1421357974_0001_m_000002_0\n",
            "2023-02-22 18:14:51,075 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2023-02-22 18:14:51,075 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2023-02-22 18:14:51,075 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2023-02-22 18:14:51,077 INFO mapred.MapTask: Processing split: file:/root/input/kms-acls.xml:0+3518\n",
            "2023-02-22 18:14:51,111 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2023-02-22 18:14:51,112 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2023-02-22 18:14:51,113 INFO mapred.MapTask: soft limit at 83886080\n",
            "2023-02-22 18:14:51,113 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2023-02-22 18:14:51,113 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2023-02-22 18:14:51,114 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2023-02-22 18:14:51,125 INFO mapred.LocalJobRunner: \n",
            "2023-02-22 18:14:51,125 INFO mapred.MapTask: Starting flush of map output\n",
            "2023-02-22 18:14:51,143 INFO mapred.Task: Task:attempt_local1421357974_0001_m_000002_0 is done. And is in the process of committing\n",
            "2023-02-22 18:14:51,151 INFO mapred.LocalJobRunner: map\n",
            "2023-02-22 18:14:51,151 INFO mapred.Task: Task 'attempt_local1421357974_0001_m_000002_0' done.\n",
            "2023-02-22 18:14:51,151 INFO mapred.Task: Final Counters for attempt_local1421357974_0001_m_000002_0: Counters: 18\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=308649\n",
            "\t\tFILE: Number of bytes written=920619\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=135\n",
            "\t\tMap output records=0\n",
            "\t\tMap output bytes=0\n",
            "\t\tMap output materialized bytes=6\n",
            "\t\tInput split bytes=94\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tSpilled Records=0\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=15\n",
            "\t\tTotal committed heap usage (bytes)=357564416\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=3518\n",
            "2023-02-22 18:14:51,152 INFO mapred.LocalJobRunner: Finishing task: attempt_local1421357974_0001_m_000002_0\n",
            "2023-02-22 18:14:51,152 INFO mapred.LocalJobRunner: Starting task: attempt_local1421357974_0001_m_000003_0\n",
            "2023-02-22 18:14:51,159 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2023-02-22 18:14:51,159 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2023-02-22 18:14:51,159 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2023-02-22 18:14:51,161 INFO mapred.MapTask: Processing split: file:/root/input/hdfs-site.xml:0+775\n",
            "2023-02-22 18:14:51,189 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2023-02-22 18:14:51,190 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2023-02-22 18:14:51,190 INFO mapred.MapTask: soft limit at 83886080\n",
            "2023-02-22 18:14:51,190 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2023-02-22 18:14:51,190 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2023-02-22 18:14:51,190 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2023-02-22 18:14:51,208 INFO mapred.LocalJobRunner: \n",
            "2023-02-22 18:14:51,208 INFO mapred.MapTask: Starting flush of map output\n",
            "2023-02-22 18:14:51,218 INFO mapred.Task: Task:attempt_local1421357974_0001_m_000003_0 is done. And is in the process of committing\n",
            "2023-02-22 18:14:51,220 INFO mapred.LocalJobRunner: map\n",
            "2023-02-22 18:14:51,220 INFO mapred.Task: Task 'attempt_local1421357974_0001_m_000003_0' done.\n",
            "2023-02-22 18:14:51,221 INFO mapred.Task: Final Counters for attempt_local1421357974_0001_m_000003_0: Counters: 18\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=310416\n",
            "\t\tFILE: Number of bytes written=920657\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=21\n",
            "\t\tMap output records=0\n",
            "\t\tMap output bytes=0\n",
            "\t\tMap output materialized bytes=6\n",
            "\t\tInput split bytes=95\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tSpilled Records=0\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=2\n",
            "\t\tTotal committed heap usage (bytes)=357564416\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=775\n",
            "2023-02-22 18:14:51,221 INFO mapred.LocalJobRunner: Finishing task: attempt_local1421357974_0001_m_000003_0\n",
            "2023-02-22 18:14:51,221 INFO mapred.LocalJobRunner: Starting task: attempt_local1421357974_0001_m_000004_0\n",
            "2023-02-22 18:14:51,223 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2023-02-22 18:14:51,223 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2023-02-22 18:14:51,223 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2023-02-22 18:14:51,227 INFO mapred.MapTask: Processing split: file:/root/input/core-site.xml:0+774\n",
            "2023-02-22 18:14:51,248 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2023-02-22 18:14:51,248 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2023-02-22 18:14:51,249 INFO mapred.MapTask: soft limit at 83886080\n",
            "2023-02-22 18:14:51,249 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2023-02-22 18:14:51,249 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2023-02-22 18:14:51,250 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2023-02-22 18:14:51,254 INFO mapred.LocalJobRunner: \n",
            "2023-02-22 18:14:51,254 INFO mapred.MapTask: Starting flush of map output\n",
            "2023-02-22 18:14:51,257 INFO mapred.Task: Task:attempt_local1421357974_0001_m_000004_0 is done. And is in the process of committing\n",
            "2023-02-22 18:14:51,261 INFO mapred.LocalJobRunner: map\n",
            "2023-02-22 18:14:51,261 INFO mapred.Task: Task 'attempt_local1421357974_0001_m_000004_0' done.\n",
            "2023-02-22 18:14:51,262 INFO mapred.Task: Final Counters for attempt_local1421357974_0001_m_000004_0: Counters: 18\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=312182\n",
            "\t\tFILE: Number of bytes written=920695\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=20\n",
            "\t\tMap output records=0\n",
            "\t\tMap output bytes=0\n",
            "\t\tMap output materialized bytes=6\n",
            "\t\tInput split bytes=95\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tSpilled Records=0\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=2\n",
            "\t\tTotal committed heap usage (bytes)=357564416\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=774\n",
            "2023-02-22 18:14:51,262 INFO mapred.LocalJobRunner: Finishing task: attempt_local1421357974_0001_m_000004_0\n",
            "2023-02-22 18:14:51,262 INFO mapred.LocalJobRunner: Starting task: attempt_local1421357974_0001_m_000005_0\n",
            "2023-02-22 18:14:51,276 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2023-02-22 18:14:51,276 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2023-02-22 18:14:51,277 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2023-02-22 18:14:51,279 INFO mapred.MapTask: Processing split: file:/root/input/mapred-site.xml:0+758\n",
            "2023-02-22 18:14:51,309 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2023-02-22 18:14:51,309 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2023-02-22 18:14:51,309 INFO mapred.MapTask: soft limit at 83886080\n",
            "2023-02-22 18:14:51,309 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2023-02-22 18:14:51,309 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2023-02-22 18:14:51,310 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2023-02-22 18:14:51,317 INFO mapred.LocalJobRunner: \n",
            "2023-02-22 18:14:51,321 INFO mapred.MapTask: Starting flush of map output\n",
            "2023-02-22 18:14:51,326 INFO mapred.Task: Task:attempt_local1421357974_0001_m_000005_0 is done. And is in the process of committing\n",
            "2023-02-22 18:14:51,330 INFO mapred.LocalJobRunner: map\n",
            "2023-02-22 18:14:51,330 INFO mapred.Task: Task 'attempt_local1421357974_0001_m_000005_0' done.\n",
            "2023-02-22 18:14:51,331 INFO mapred.Task: Final Counters for attempt_local1421357974_0001_m_000005_0: Counters: 18\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=313932\n",
            "\t\tFILE: Number of bytes written=920733\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=21\n",
            "\t\tMap output records=0\n",
            "\t\tMap output bytes=0\n",
            "\t\tMap output materialized bytes=6\n",
            "\t\tInput split bytes=97\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tSpilled Records=0\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=357564416\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=758\n",
            "2023-02-22 18:14:51,331 INFO mapred.LocalJobRunner: Finishing task: attempt_local1421357974_0001_m_000005_0\n",
            "2023-02-22 18:14:51,331 INFO mapred.LocalJobRunner: Starting task: attempt_local1421357974_0001_m_000006_0\n",
            "2023-02-22 18:14:51,332 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2023-02-22 18:14:51,333 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2023-02-22 18:14:51,333 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2023-02-22 18:14:51,335 INFO mapred.MapTask: Processing split: file:/root/input/yarn-site.xml:0+690\n",
            "2023-02-22 18:14:51,357 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2023-02-22 18:14:51,357 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2023-02-22 18:14:51,357 INFO mapred.MapTask: soft limit at 83886080\n",
            "2023-02-22 18:14:51,357 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2023-02-22 18:14:51,357 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2023-02-22 18:14:51,358 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2023-02-22 18:14:51,362 INFO mapred.LocalJobRunner: \n",
            "2023-02-22 18:14:51,362 INFO mapred.MapTask: Starting flush of map output\n",
            "2023-02-22 18:14:51,366 INFO mapred.Task: Task:attempt_local1421357974_0001_m_000006_0 is done. And is in the process of committing\n",
            "2023-02-22 18:14:51,368 INFO mapred.LocalJobRunner: map\n",
            "2023-02-22 18:14:51,370 INFO mapred.Task: Task 'attempt_local1421357974_0001_m_000006_0' done.\n",
            "2023-02-22 18:14:51,371 INFO mapred.Task: Final Counters for attempt_local1421357974_0001_m_000006_0: Counters: 18\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=315102\n",
            "\t\tFILE: Number of bytes written=920771\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=19\n",
            "\t\tMap output records=0\n",
            "\t\tMap output bytes=0\n",
            "\t\tMap output materialized bytes=6\n",
            "\t\tInput split bytes=95\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tSpilled Records=0\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=4\n",
            "\t\tTotal committed heap usage (bytes)=357564416\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=690\n",
            "2023-02-22 18:14:51,371 INFO mapred.LocalJobRunner: Finishing task: attempt_local1421357974_0001_m_000006_0\n",
            "2023-02-22 18:14:51,371 INFO mapred.LocalJobRunner: Starting task: attempt_local1421357974_0001_m_000007_0\n",
            "2023-02-22 18:14:51,375 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2023-02-22 18:14:51,375 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2023-02-22 18:14:51,376 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2023-02-22 18:14:51,391 INFO mapred.MapTask: Processing split: file:/root/input/hdfs-rbf-site.xml:0+683\n",
            "2023-02-22 18:14:51,408 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2023-02-22 18:14:51,408 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2023-02-22 18:14:51,408 INFO mapred.MapTask: soft limit at 83886080\n",
            "2023-02-22 18:14:51,408 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2023-02-22 18:14:51,408 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2023-02-22 18:14:51,409 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2023-02-22 18:14:51,413 INFO mapred.LocalJobRunner: \n",
            "2023-02-22 18:14:51,413 INFO mapred.MapTask: Starting flush of map output\n",
            "2023-02-22 18:14:51,416 INFO mapred.Task: Task:attempt_local1421357974_0001_m_000007_0 is done. And is in the process of committing\n",
            "2023-02-22 18:14:51,419 INFO mapred.LocalJobRunner: map\n",
            "2023-02-22 18:14:51,419 INFO mapred.Task: Task 'attempt_local1421357974_0001_m_000007_0' done.\n",
            "2023-02-22 18:14:51,419 INFO mapred.Task: Final Counters for attempt_local1421357974_0001_m_000007_0: Counters: 18\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=316265\n",
            "\t\tFILE: Number of bytes written=920809\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=20\n",
            "\t\tMap output records=0\n",
            "\t\tMap output bytes=0\n",
            "\t\tMap output materialized bytes=6\n",
            "\t\tInput split bytes=99\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tSpilled Records=0\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=357564416\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=683\n",
            "2023-02-22 18:14:51,419 INFO mapred.LocalJobRunner: Finishing task: attempt_local1421357974_0001_m_000007_0\n",
            "2023-02-22 18:14:51,420 INFO mapred.LocalJobRunner: Starting task: attempt_local1421357974_0001_m_000008_0\n",
            "2023-02-22 18:14:51,421 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2023-02-22 18:14:51,421 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2023-02-22 18:14:51,422 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2023-02-22 18:14:51,423 INFO mapred.MapTask: Processing split: file:/root/input/kms-site.xml:0+682\n",
            "2023-02-22 18:14:51,445 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2023-02-22 18:14:51,445 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2023-02-22 18:14:51,445 INFO mapred.MapTask: soft limit at 83886080\n",
            "2023-02-22 18:14:51,445 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2023-02-22 18:14:51,445 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2023-02-22 18:14:51,446 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2023-02-22 18:14:51,450 INFO mapred.LocalJobRunner: \n",
            "2023-02-22 18:14:51,450 INFO mapred.MapTask: Starting flush of map output\n",
            "2023-02-22 18:14:51,454 INFO mapred.Task: Task:attempt_local1421357974_0001_m_000008_0 is done. And is in the process of committing\n",
            "2023-02-22 18:14:51,457 INFO mapred.LocalJobRunner: map\n",
            "2023-02-22 18:14:51,457 INFO mapred.Task: Task 'attempt_local1421357974_0001_m_000008_0' done.\n",
            "2023-02-22 18:14:51,457 INFO mapred.Task: Final Counters for attempt_local1421357974_0001_m_000008_0: Counters: 18\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=317427\n",
            "\t\tFILE: Number of bytes written=920847\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=20\n",
            "\t\tMap output records=0\n",
            "\t\tMap output bytes=0\n",
            "\t\tMap output materialized bytes=6\n",
            "\t\tInput split bytes=94\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tSpilled Records=0\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=2\n",
            "\t\tTotal committed heap usage (bytes)=357564416\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=682\n",
            "2023-02-22 18:14:51,457 INFO mapred.LocalJobRunner: Finishing task: attempt_local1421357974_0001_m_000008_0\n",
            "2023-02-22 18:14:51,458 INFO mapred.LocalJobRunner: Starting task: attempt_local1421357974_0001_m_000009_0\n",
            "2023-02-22 18:14:51,460 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2023-02-22 18:14:51,460 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2023-02-22 18:14:51,468 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2023-02-22 18:14:51,470 INFO mapred.MapTask: Processing split: file:/root/input/httpfs-site.xml:0+620\n",
            "2023-02-22 18:14:51,491 INFO mapreduce.Job: Job job_local1421357974_0001 running in uber mode : false\n",
            "2023-02-22 18:14:51,493 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2023-02-22 18:14:51,493 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2023-02-22 18:14:51,493 INFO mapred.MapTask: soft limit at 83886080\n",
            "2023-02-22 18:14:51,493 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2023-02-22 18:14:51,493 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2023-02-22 18:14:51,494 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2023-02-22 18:14:51,502 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2023-02-22 18:14:51,511 INFO mapred.LocalJobRunner: \n",
            "2023-02-22 18:14:51,512 INFO mapred.MapTask: Starting flush of map output\n",
            "2023-02-22 18:14:51,518 INFO mapred.Task: Task:attempt_local1421357974_0001_m_000009_0 is done. And is in the process of committing\n",
            "2023-02-22 18:14:51,521 INFO mapred.LocalJobRunner: map\n",
            "2023-02-22 18:14:51,521 INFO mapred.Task: Task 'attempt_local1421357974_0001_m_000009_0' done.\n",
            "2023-02-22 18:14:51,522 INFO mapred.Task: Final Counters for attempt_local1421357974_0001_m_000009_0: Counters: 18\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=318527\n",
            "\t\tFILE: Number of bytes written=920885\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=17\n",
            "\t\tMap output records=0\n",
            "\t\tMap output bytes=0\n",
            "\t\tMap output materialized bytes=6\n",
            "\t\tInput split bytes=97\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tSpilled Records=0\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=357564416\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=620\n",
            "2023-02-22 18:14:51,522 INFO mapred.LocalJobRunner: Finishing task: attempt_local1421357974_0001_m_000009_0\n",
            "2023-02-22 18:14:51,522 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2023-02-22 18:14:51,527 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2023-02-22 18:14:51,528 INFO mapred.LocalJobRunner: Starting task: attempt_local1421357974_0001_r_000000_0\n",
            "2023-02-22 18:14:51,543 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2023-02-22 18:14:51,543 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2023-02-22 18:14:51,543 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2023-02-22 18:14:51,546 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@59f12e11\n",
            "2023-02-22 18:14:51,548 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2023-02-22 18:14:51,572 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2384042240, maxSingleShuffleLimit=596010560, mergeThreshold=1573467904, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2023-02-22 18:14:51,574 INFO reduce.EventFetcher: attempt_local1421357974_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2023-02-22 18:14:51,630 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1421357974_0001_m_000000_0 decomp: 21 len: 25 to MEMORY\n",
            "2023-02-22 18:14:51,635 INFO reduce.InMemoryMapOutput: Read 21 bytes from map-output for attempt_local1421357974_0001_m_000000_0\n",
            "2023-02-22 18:14:51,639 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 21, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->21\n",
            "2023-02-22 18:14:51,644 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1421357974_0001_m_000003_0 decomp: 2 len: 6 to MEMORY\n",
            "2023-02-22 18:14:51,645 INFO reduce.InMemoryMapOutput: Read 2 bytes from map-output for attempt_local1421357974_0001_m_000003_0\n",
            "2023-02-22 18:14:51,645 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 2, inMemoryMapOutputs.size() -> 2, commitMemory -> 21, usedMemory ->23\n",
            "2023-02-22 18:14:51,647 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1421357974_0001_m_000006_0 decomp: 2 len: 6 to MEMORY\n",
            "2023-02-22 18:14:51,655 INFO reduce.InMemoryMapOutput: Read 2 bytes from map-output for attempt_local1421357974_0001_m_000006_0\n",
            "2023-02-22 18:14:51,656 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 2, inMemoryMapOutputs.size() -> 3, commitMemory -> 23, usedMemory ->25\n",
            "2023-02-22 18:14:51,658 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1421357974_0001_m_000001_0 decomp: 20 len: 24 to MEMORY\n",
            "2023-02-22 18:14:51,659 INFO reduce.InMemoryMapOutput: Read 20 bytes from map-output for attempt_local1421357974_0001_m_000001_0\n",
            "2023-02-22 18:14:51,659 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 20, inMemoryMapOutputs.size() -> 4, commitMemory -> 25, usedMemory ->45\n",
            "2023-02-22 18:14:51,661 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1421357974_0001_m_000004_0 decomp: 2 len: 6 to MEMORY\n",
            "2023-02-22 18:14:51,662 INFO reduce.InMemoryMapOutput: Read 2 bytes from map-output for attempt_local1421357974_0001_m_000004_0\n",
            "2023-02-22 18:14:51,662 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 2, inMemoryMapOutputs.size() -> 5, commitMemory -> 45, usedMemory ->47\n",
            "2023-02-22 18:14:51,663 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1421357974_0001_m_000007_0 decomp: 2 len: 6 to MEMORY\n",
            "2023-02-22 18:14:51,664 INFO reduce.InMemoryMapOutput: Read 2 bytes from map-output for attempt_local1421357974_0001_m_000007_0\n",
            "2023-02-22 18:14:51,664 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 2, inMemoryMapOutputs.size() -> 6, commitMemory -> 47, usedMemory ->49\n",
            "2023-02-22 18:14:51,669 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1421357974_0001_m_000005_0 decomp: 2 len: 6 to MEMORY\n",
            "2023-02-22 18:14:51,669 INFO reduce.InMemoryMapOutput: Read 2 bytes from map-output for attempt_local1421357974_0001_m_000005_0\n",
            "2023-02-22 18:14:51,669 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 2, inMemoryMapOutputs.size() -> 7, commitMemory -> 49, usedMemory ->51\n",
            "2023-02-22 18:14:51,671 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1421357974_0001_m_000008_0 decomp: 2 len: 6 to MEMORY\n",
            "2023-02-22 18:14:51,672 INFO reduce.InMemoryMapOutput: Read 2 bytes from map-output for attempt_local1421357974_0001_m_000008_0\n",
            "2023-02-22 18:14:51,672 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 2, inMemoryMapOutputs.size() -> 8, commitMemory -> 51, usedMemory ->53\n",
            "2023-02-22 18:14:51,674 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1421357974_0001_m_000009_0 decomp: 2 len: 6 to MEMORY\n",
            "2023-02-22 18:14:51,675 INFO reduce.InMemoryMapOutput: Read 2 bytes from map-output for attempt_local1421357974_0001_m_000009_0\n",
            "2023-02-22 18:14:51,675 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 2, inMemoryMapOutputs.size() -> 9, commitMemory -> 53, usedMemory ->55\n",
            "2023-02-22 18:14:51,677 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1421357974_0001_m_000002_0 decomp: 2 len: 6 to MEMORY\n",
            "2023-02-22 18:14:51,677 INFO reduce.InMemoryMapOutput: Read 2 bytes from map-output for attempt_local1421357974_0001_m_000002_0\n",
            "2023-02-22 18:14:51,677 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 2, inMemoryMapOutputs.size() -> 10, commitMemory -> 55, usedMemory ->57\n",
            "2023-02-22 18:14:51,678 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2023-02-22 18:14:51,680 INFO mapred.LocalJobRunner: 10 / 10 copied.\n",
            "2023-02-22 18:14:51,680 INFO reduce.MergeManagerImpl: finalMerge called with 10 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2023-02-22 18:14:51,691 INFO mapred.Merger: Merging 10 sorted segments\n",
            "2023-02-22 18:14:51,692 INFO mapred.Merger: Down to the last merge-pass, with 2 segments left of total size: 20 bytes\n",
            "2023-02-22 18:14:51,693 INFO reduce.MergeManagerImpl: Merged 10 segments, 57 bytes to disk to satisfy reduce memory limit\n",
            "2023-02-22 18:14:51,695 INFO reduce.MergeManagerImpl: Merging 1 files, 43 bytes from disk\n",
            "2023-02-22 18:14:51,696 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2023-02-22 18:14:51,696 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2023-02-22 18:14:51,697 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 29 bytes\n",
            "2023-02-22 18:14:51,698 INFO mapred.LocalJobRunner: 10 / 10 copied.\n",
            "2023-02-22 18:14:51,722 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2023-02-22 18:14:51,724 INFO mapred.Task: Task:attempt_local1421357974_0001_r_000000_0 is done. And is in the process of committing\n",
            "2023-02-22 18:14:51,725 INFO mapred.LocalJobRunner: 10 / 10 copied.\n",
            "2023-02-22 18:14:51,725 INFO mapred.Task: Task attempt_local1421357974_0001_r_000000_0 is allowed to commit now\n",
            "2023-02-22 18:14:51,727 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1421357974_0001_r_000000_0' to file:/content/grep-temp-1564932779\n",
            "2023-02-22 18:14:51,728 INFO mapred.LocalJobRunner: reduce > reduce\n",
            "2023-02-22 18:14:51,729 INFO mapred.Task: Task 'attempt_local1421357974_0001_r_000000_0' done.\n",
            "2023-02-22 18:14:51,729 INFO mapred.Task: Final Counters for attempt_local1421357974_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=318987\n",
            "\t\tFILE: Number of bytes written=921075\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=2\n",
            "\t\tReduce shuffle bytes=97\n",
            "\t\tReduce input records=2\n",
            "\t\tReduce output records=2\n",
            "\t\tSpilled Records=2\n",
            "\t\tShuffled Maps =10\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=10\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=357564416\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=147\n",
            "2023-02-22 18:14:51,729 INFO mapred.LocalJobRunner: Finishing task: attempt_local1421357974_0001_r_000000_0\n",
            "2023-02-22 18:14:51,729 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2023-02-22 18:14:52,502 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2023-02-22 18:14:52,503 INFO mapreduce.Job: Job job_local1421357974_0001 completed successfully\n",
            "2023-02-22 18:14:52,527 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=3429560\n",
            "\t\tFILE: Number of bytes written=10128197\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=792\n",
            "\t\tMap output records=23\n",
            "\t\tMap output bytes=390\n",
            "\t\tMap output materialized bytes=97\n",
            "\t\tInput split bytes=969\n",
            "\t\tCombine input records=23\n",
            "\t\tCombine output records=2\n",
            "\t\tReduce input groups=2\n",
            "\t\tReduce shuffle bytes=97\n",
            "\t\tReduce input records=2\n",
            "\t\tReduce output records=2\n",
            "\t\tSpilled Records=4\n",
            "\t\tShuffled Maps =10\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=10\n",
            "\t\tGC time elapsed (ms)=84\n",
            "\t\tTotal committed heap usage (bytes)=3933208576\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=29478\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=147\n",
            "2023-02-22 18:14:52,566 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2023-02-22 18:14:52,587 INFO input.FileInputFormat: Total input files to process : 1\n",
            "2023-02-22 18:14:52,590 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2023-02-22 18:14:52,643 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1510136550_0002\n",
            "2023-02-22 18:14:52,643 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2023-02-22 18:14:52,803 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2023-02-22 18:14:52,803 INFO mapreduce.Job: Running job: job_local1510136550_0002\n",
            "2023-02-22 18:14:52,804 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2023-02-22 18:14:52,804 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2023-02-22 18:14:52,804 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2023-02-22 18:14:52,805 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
            "2023-02-22 18:14:52,813 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2023-02-22 18:14:52,814 INFO mapred.LocalJobRunner: Starting task: attempt_local1510136550_0002_m_000000_0\n",
            "2023-02-22 18:14:52,819 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2023-02-22 18:14:52,819 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2023-02-22 18:14:52,819 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2023-02-22 18:14:52,824 INFO mapred.MapTask: Processing split: file:/content/grep-temp-1564932779/part-r-00000:0+135\n",
            "2023-02-22 18:14:52,869 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2023-02-22 18:14:52,869 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2023-02-22 18:14:52,869 INFO mapred.MapTask: soft limit at 83886080\n",
            "2023-02-22 18:14:52,869 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2023-02-22 18:14:52,869 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2023-02-22 18:14:52,871 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2023-02-22 18:14:52,880 INFO mapred.LocalJobRunner: \n",
            "2023-02-22 18:14:52,881 INFO mapred.MapTask: Starting flush of map output\n",
            "2023-02-22 18:14:52,881 INFO mapred.MapTask: Spilling map output\n",
            "2023-02-22 18:14:52,881 INFO mapred.MapTask: bufstart = 0; bufend = 33; bufvoid = 104857600\n",
            "2023-02-22 18:14:52,881 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214392(104857568); length = 5/6553600\n",
            "2023-02-22 18:14:52,885 INFO mapred.MapTask: Finished spill 0\n",
            "2023-02-22 18:14:52,889 INFO mapred.Task: Task:attempt_local1510136550_0002_m_000000_0 is done. And is in the process of committing\n",
            "2023-02-22 18:14:52,903 INFO mapred.LocalJobRunner: map\n",
            "2023-02-22 18:14:52,903 INFO mapred.Task: Task 'attempt_local1510136550_0002_m_000000_0' done.\n",
            "2023-02-22 18:14:52,905 INFO mapred.Task: Final Counters for attempt_local1510136550_0002_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=600291\n",
            "\t\tFILE: Number of bytes written=1839301\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=2\n",
            "\t\tMap output records=2\n",
            "\t\tMap output bytes=33\n",
            "\t\tMap output materialized bytes=43\n",
            "\t\tInput split bytes=112\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=2\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=12\n",
            "\t\tTotal committed heap usage (bytes)=357564416\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=147\n",
            "2023-02-22 18:14:52,906 INFO mapred.LocalJobRunner: Finishing task: attempt_local1510136550_0002_m_000000_0\n",
            "2023-02-22 18:14:52,906 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2023-02-22 18:14:52,908 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2023-02-22 18:14:52,908 INFO mapred.LocalJobRunner: Starting task: attempt_local1510136550_0002_r_000000_0\n",
            "2023-02-22 18:14:52,919 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2023-02-22 18:14:52,919 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2023-02-22 18:14:52,920 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2023-02-22 18:14:52,920 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@44ddcd56\n",
            "2023-02-22 18:14:52,921 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2023-02-22 18:14:52,922 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2384042240, maxSingleShuffleLimit=596010560, mergeThreshold=1573467904, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2023-02-22 18:14:52,924 INFO reduce.EventFetcher: attempt_local1510136550_0002_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2023-02-22 18:14:52,927 INFO reduce.LocalFetcher: localfetcher#2 about to shuffle output of map attempt_local1510136550_0002_m_000000_0 decomp: 39 len: 43 to MEMORY\n",
            "2023-02-22 18:14:52,928 INFO reduce.InMemoryMapOutput: Read 39 bytes from map-output for attempt_local1510136550_0002_m_000000_0\n",
            "2023-02-22 18:14:52,929 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 39, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->39\n",
            "2023-02-22 18:14:52,930 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2023-02-22 18:14:52,931 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2023-02-22 18:14:52,931 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2023-02-22 18:14:52,932 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2023-02-22 18:14:52,932 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 29 bytes\n",
            "2023-02-22 18:14:52,933 INFO reduce.MergeManagerImpl: Merged 1 segments, 39 bytes to disk to satisfy reduce memory limit\n",
            "2023-02-22 18:14:52,933 INFO reduce.MergeManagerImpl: Merging 1 files, 43 bytes from disk\n",
            "2023-02-22 18:14:52,933 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2023-02-22 18:14:52,933 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2023-02-22 18:14:52,934 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 29 bytes\n",
            "2023-02-22 18:14:52,934 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2023-02-22 18:14:52,937 INFO mapred.Task: Task:attempt_local1510136550_0002_r_000000_0 is done. And is in the process of committing\n",
            "2023-02-22 18:14:52,938 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2023-02-22 18:14:52,938 INFO mapred.Task: Task attempt_local1510136550_0002_r_000000_0 is allowed to commit now\n",
            "2023-02-22 18:14:52,939 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1510136550_0002_r_000000_0' to file:/root/grep_example\n",
            "2023-02-22 18:14:52,940 INFO mapred.LocalJobRunner: reduce > reduce\n",
            "2023-02-22 18:14:52,940 INFO mapred.Task: Task 'attempt_local1510136550_0002_r_000000_0' done.\n",
            "2023-02-22 18:14:52,940 INFO mapred.Task: Final Counters for attempt_local1510136550_0002_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=600409\n",
            "\t\tFILE: Number of bytes written=1839378\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=2\n",
            "\t\tReduce shuffle bytes=43\n",
            "\t\tReduce input records=2\n",
            "\t\tReduce output records=2\n",
            "\t\tSpilled Records=2\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=357564416\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=34\n",
            "2023-02-22 18:14:52,940 INFO mapred.LocalJobRunner: Finishing task: attempt_local1510136550_0002_r_000000_0\n",
            "2023-02-22 18:14:52,940 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2023-02-22 18:14:53,804 INFO mapreduce.Job: Job job_local1510136550_0002 running in uber mode : false\n",
            "2023-02-22 18:14:53,804 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2023-02-22 18:14:53,804 INFO mapreduce.Job: Job job_local1510136550_0002 completed successfully\n",
            "2023-02-22 18:14:53,807 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=1200700\n",
            "\t\tFILE: Number of bytes written=3678679\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=2\n",
            "\t\tMap output records=2\n",
            "\t\tMap output bytes=33\n",
            "\t\tMap output materialized bytes=43\n",
            "\t\tInput split bytes=112\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=2\n",
            "\t\tReduce shuffle bytes=43\n",
            "\t\tReduce input records=2\n",
            "\t\tReduce output records=2\n",
            "\t\tSpilled Records=4\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=12\n",
            "\t\tTotal committed heap usage (bytes)=715128832\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=147\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=34\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.   Los resultados se almacenan en el directorio de salida (~/grep_example/) y se pueden verificar ejecutando cat el directorio de salida:\n",
        "\n"
      ],
      "metadata": {
        "id": "uU55dsBMwpWw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat ~/grep_example/*"
      ],
      "metadata": {
        "id": "R-VTuKtnwo5o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6083c30-ddb2-4c42-f838-f312a402fe41"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22\tallowed.\n",
            "1\tallowed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Es interesante probar otros ejemplos contenidos en el jar de ejemplos de hadoop. [Hadoop Map Reduce Examples](http://svn.apache.org/viewvc/hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/)\n",
        "\n",
        "Puedes ejecutar el siguiente comando para obtener una referencia de los ejemplos disponibles en esta distribución"
      ],
      "metadata": {
        "id": "RvjHvQlww0JZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!/usr/local/hadoop-3.3.4/bin/hadoop jar /usr/local/hadoop-3.3.4/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.4.jar"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4B29TgZAsOD8",
        "outputId": "67f54a09-aba0-4d42-df45-ef06ebb83e39"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An example program must be given as the first argument.\n",
            "Valid program names are:\n",
            "  aggregatewordcount: An Aggregate based map/reduce program that counts the words in the input files.\n",
            "  aggregatewordhist: An Aggregate based map/reduce program that computes the histogram of the words in the input files.\n",
            "  bbp: A map/reduce program that uses Bailey-Borwein-Plouffe to compute exact digits of Pi.\n",
            "  dbcount: An example job that count the pageview counts from a database.\n",
            "  distbbp: A map/reduce program that uses a BBP-type formula to compute exact bits of Pi.\n",
            "  grep: A map/reduce program that counts the matches of a regex in the input.\n",
            "  join: A job that effects a join over sorted, equally partitioned datasets\n",
            "  multifilewc: A job that counts words from several files.\n",
            "  pentomino: A map/reduce tile laying program to find solutions to pentomino problems.\n",
            "  pi: A map/reduce program that estimates Pi using a quasi-Monte Carlo method.\n",
            "  randomtextwriter: A map/reduce program that writes 10GB of random textual data per node.\n",
            "  randomwriter: A map/reduce program that writes 10GB of random data per node.\n",
            "  secondarysort: An example defining a secondary sort to the reduce.\n",
            "  sort: A map/reduce program that sorts the data written by the random writer.\n",
            "  sudoku: A sudoku solver.\n",
            "  teragen: Generate data for the terasort\n",
            "  terasort: Run the terasort\n",
            "  teravalidate: Checking results of terasort\n",
            "  wordcount: A map/reduce program that counts the words in the input files.\n",
            "  wordmean: A map/reduce program that counts the average length of the words in the input files.\n",
            "  wordmedian: A map/reduce program that counts the median length of the words in the input files.\n",
            "  wordstandarddeviation: A map/reduce program that counts the standard deviation of the length of the words in the input files.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HDFS\n",
        "\n",
        "- Es un sistema de archivos distribuido.\n",
        "- HDFS es altamente tolerante a fallos y está diseñado para ser desplegado en hardware de bajo coste.\n",
        "- HDFS es adecuado para aplicaciones que manejan grandes conjuntos de datos.\n",
        "- HDFS proporciona interfaces para acercar las aplicaciones al lugar donde se encuentran los datos. El cálculo es mucho más eficiente cuando el tamaño del conjunto de datos es enorme.\n",
        "- HDFS consiste en un único NameNode con un número de DataNodes que gestionan el almacenamiento.\n",
        "- HDFS dispone de un espacio de nombres del sistema de archivos y permite que los datos del usuario se almacenen en archivos.\n",
        "  1. El NameNode divide un archivo en bloques almacenados en DataNodes.\n",
        "  2. El NameNode ejecuta operaciones como abrir, cerrar y renombrar archivos y directorios.\n",
        "  3. El NameNode secundario almacena la información del NameNode.\n",
        "  4.  Los DataNodes gestionan la creación, eliminación y replicación de bloques siguiendo instrucciones del NameNode.\n",
        "  5. La ubicación de las réplicas está optimizada para la fiabilidad de los datos, la disponibilidad y la utilización del ancho de banda de la red.\n",
        "  6. Los datos del usuario nunca pasan por el NameNode.\n",
        "\n",
        "- Los archivos en HDFS son de una sola escritura y tienen estrictamente un único proceso escritor en cualquier momento.\n",
        "\n",
        "- El DataNode no tiene conocimiento de los archivos de HDFS."
      ],
      "metadata": {
        "id": "UO1DRA0jvnzF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTA: Las siguientes sentencias únicamente sirven para probar comandos básicos de HDFS no para gestionar una Infraestructura que en Google Colab no existe, en este caso el sistema de archivos HDFS es el mismo que el local** "
      ],
      "metadata": {
        "id": "dJbfP6ZRvyKO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Accesibilidad\n",
        "\n",
        "Todos los  [comandos HDFS](http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/FileSystemShell.html)  se invocan mediante el script *bin/hdfs* :\n",
        "```shell\n",
        "hdfs [SHELL_OPTIONS] COMMAND [GENERIC_OPTIONS] [COMMAND_OPTIONS]\n",
        "```\n",
        "## Gestión de ficheros y directorios\n",
        "```shell\n",
        "hdfs dfs -ls -h -R # lista de subdirectorios recuriva\n",
        "hdfs dfs -cp  # Copia de ficheros\n",
        "hdfs dfs -mv  # Movimiento de ficheros\n",
        "hdfs dfs -mkdir /foodir # Crear directorio llamado /foodir\t\n",
        "hdfs dfs -rm -r /foodir   # Borrar un directorio /foodir\t\n",
        "hdfs dfs -cat /foodir/myfile.txt # Ver los contenidos de un fichero /foodir/myfile.txt\t\n",
        "```"
      ],
      "metadata": {
        "id": "HfPCzSN6v6P0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Listar el contenido de un directorio\n"
      ],
      "metadata": {
        "id": "ChwKiwl_wmLx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!/usr/local/hadoop-3.3.4/bin/hdfs dfs -ls /content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RvLjTqGYkDgp",
        "outputId": "bf55d0d7-6cba-4154-a46b-2162a7395f77"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 4 items\n",
            "drwxr-xr-x   - root root       4096 2023-02-21 14:37 /content/.config\n",
            "drwxr-xr-x   - 1024 1024       4096 2022-07-29 13:44 /content/hadoop-3.3.4\n",
            "-rw-r--r--   1 root root  695457782 2022-07-29 18:11 /content/hadoop-3.3.4.tar.gz\n",
            "drwxr-xr-x   - root root       4096 2023-02-21 14:37 /content/sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Crear el directorio prueba"
      ],
      "metadata": {
        "id": "7pdcP5rdw3e-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "/usr/local/hadoop-3.3.4/bin/hdfs dfs -mkdir prueba\n",
        "/usr/local/hadoop-3.3.4/bin/hdfs dfs -ls "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q67sE80mmibi",
        "outputId": "43923a38-df36-4eb1-ffcc-734155234689"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 5 items\n",
            "drwxr-xr-x   - root root       4096 2023-02-21 14:37 .config\n",
            "drwxr-xr-x   - 1024 1024       4096 2022-07-29 13:44 hadoop-3.3.4\n",
            "-rw-r--r--   1 root root  695457782 2022-07-29 18:11 hadoop-3.3.4.tar.gz\n",
            "drwxr-xr-x   - root root       4096 2023-02-22 18:14 prueba\n",
            "drwxr-xr-x   - root root       4096 2023-02-21 14:37 sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Copar en HDFS:\n",
        "\n",
        "```bash\n",
        "hdfs dfs -put user.txt\n",
        "```\n",
        "\n",
        "*   Comprobar:\n",
        "```bash\n",
        "hdfs dfs -ls -R \n",
        "hdfs dfs -cat user.txt \n",
        "hdfs dfs -tail user.txt \n",
        "```\n",
        "- Crear un fichero local :"
      ],
      "metadata": {
        "id": "uUEc8iWXxkSz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "echo \"Ejemplo de HDFS\" > user.txt\n",
        "echo `date` >> user.txt \n",
        "cat user.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xowJIi-yztW",
        "outputId": "a36046ba-f7b5-4bff-f59c-935620684b2b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ejemplo de HDFS\n",
            "Wed 22 Feb 2023 06:15:02 PM UTC\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Colocarlo (copiarlo)  en el directorio *prueba*"
      ],
      "metadata": {
        "id": "CssXQcnLy34L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!/usr/local/hadoop-3.3.4/bin/hdfs dfs -put user.txt prueba/"
      ],
      "metadata": {
        "id": "c-Z6SfyHy4jQ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Mostrar su contenido"
      ],
      "metadata": {
        "id": "an7ImoIqzBBP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!/usr/local/hadoop-3.3.4/bin/hdfs dfs -cat prueba/user.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2rN43iUIzBwb",
        "outputId": "3c69cf07-6d73-45b7-9ce8-3b3431d9d1a2"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ejemplo de HDFS\n",
            "Wed 22 Feb 2023 06:15:02 PM UTC\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejercicio 1\n",
        "1. Cree un directorio `files` en HDFS.\n",
        "2. Listar el contenido de un directorio /.\n",
        "3. Cargar el archivo today.txt en HDFS.\n",
        "```bash\n",
        "date > hoy.txt\n",
        "whoami >> hoy.txt\n",
        "```\n",
        "4. Mostrar el contenido del archivo `hoy.txt`.\n",
        "5. Copiar el archivo `hoy.txt` del origen al directorio `files`.\n",
        "6. Copiar el archivo `jps.txt` desde/hacia el sistema de archivos local a HDFS\n",
        "```bash\n",
        "jps > jps.txt\n",
        "```\n",
        "7. Mover el archivo `jps.txt` de la fuente a `files`.\n",
        "8. Eliminar el archivo `today.txt` del directorio principal en HDFS.\n",
        "9. Mostrar las últimas líneas de `jps.txt`.\n"
      ],
      "metadata": {
        "id": "kdpKi-H_zPbw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1\n",
        "!/usr/local/hadoop-3.3.4/bin/hdfs dfs -mkdir files\n",
        "\n",
        "#2\n",
        "!/usr/local/hadoop-3.3.4/bin/hdfs dfs -ls /"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J3iEQzUTUnFZ",
        "outputId": "7dc38f31-c9f9-4b0e-efc9-801c0c2a6372"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 26 items\n",
            "-rwxr-xr-x   1 root root          0 2023-02-22 18:10 /.dockerenv\n",
            "-rw-r--r--   1 root root      16047 2022-12-14 20:29 /NGC-DL-CONTAINER-LICENSE\n",
            "drwxr-xr-x   - root root       4096 2023-02-21 14:37 /bin\n",
            "drwxr-xr-x   - root root       4096 2020-04-15 11:09 /boot\n",
            "drwxr-xr-x   - root root       4096 2023-02-22 18:19 /content\n",
            "drwxr-xr-x   - root root       4096 2023-02-21 14:49 /datalab\n",
            "drwxr-xr-x   - root root        360 2023-02-22 18:10 /dev\n",
            "drwxr-xr-x   - root root       4096 2023-02-22 18:10 /etc\n",
            "drwxr-xr-x   - root root       4096 2020-04-15 11:09 /home\n",
            "drwxr-xr-x   - root root       4096 2023-02-21 14:31 /lib\n",
            "drwxr-xr-x   - root root       4096 2023-02-21 14:25 /lib32\n",
            "drwxr-xr-x   - root root       4096 2022-11-30 02:06 /lib64\n",
            "drwxr-xr-x   - root root       4096 2022-11-30 02:04 /libx32\n",
            "drwxr-xr-x   - root root       4096 2022-11-30 02:04 /media\n",
            "drwxr-xr-x   - root root       4096 2022-11-30 02:04 /mnt\n",
            "drwxr-xr-x   - root root       4096 2023-02-21 14:50 /opt\n",
            "dr-xr-xr-x   - root root          0 2023-02-22 18:10 /proc\n",
            "drwx------   - root root       4096 2023-02-22 18:14 /root\n",
            "drwxr-xr-x   - root root       4096 2023-02-21 14:32 /run\n",
            "drwxr-xr-x   - root root       4096 2023-02-22 18:10 /sbin\n",
            "drwxr-xr-x   - root root       4096 2022-11-30 02:04 /srv\n",
            "dr-xr-xr-x   - root root          0 2023-02-22 18:10 /sys\n",
            "drwxrwxrwt   - root root       4096 2023-02-22 18:14 /tmp\n",
            "drwxr-xr-x   - root root       4096 2023-02-21 14:49 /tools\n",
            "drwxr-xr-x   - root root       4096 2023-02-21 14:50 /usr\n",
            "drwxr-xr-x   - root root       4096 2023-02-21 14:49 /var\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "date > hoy.txt\n",
        "whoami >> hoy.txt\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0yFx_xWvWimL"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!/usr/local/hadoop-3.3.4/bin/hdfs dfs -put hoy.txt\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhI6BBBoXBYk",
        "outputId": "7a0eae93-0f89-4e37-ef1c-8a509ad77c5c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "put: `hoy.txt': File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#4º \n",
        "!/usr/local/hadoop-3.3.4/bin/hdfs dfs -cat hoy.txt\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njsEgg_vXHYq",
        "outputId": "fb720db9-e9db-4b85-c6e5-750b36a6e2fa"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed 22 Feb 2023 06:21:36 PM UTC\n",
            "root\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!/usr/local/hadoop-3.3.4/bin/hdfs dfs -cp hoy.txt files/hoy.txt\n",
        "\n",
        "!/usr/local/hadoop-3.3.4/bin/hdfs dfs -ls files\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PEtOYhv1Xzdc",
        "outputId": "8f6a6012-f6df-44b5-cb35-b4bedeed04af"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1 items\n",
            "-rw-r--r--   1 root root         37 2023-02-22 18:29 files/hoy.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#6\n",
        "!jps > jps.txt\n",
        "#7\n",
        "!/usr/local/hadoop-3.3.4/bin/hdfs dfs -put jps.txt files/\n"
      ],
      "metadata": {
        "id": "7GCZ-V_lZOT_"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!/usr/local/hadoop-3.3.4/bin/hdfs dfs -ls files"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ss6jdZ0Zgin",
        "outputId": "c08bc9ef-1e05-4957-9db7-10374d78c102"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2 items\n",
            "-rw-r--r--   1 root root         37 2023-02-22 18:29 files/hoy.txt\n",
            "-rw-r--r--   1 root root          9 2023-02-22 18:36 files/jps.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#8\n",
        "c -rm files/hoy.txt\n",
        "!/usr/local/hadoop-3.3.4/bin/hdfs dfs -ls files"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sr7loTDnalHT",
        "outputId": "c7f36f36-bef5-497b-8c6a-a6fec3a70bfe"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-22 18:38:22,667 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "Deleted files/hoy.txt\n",
            "Found 1 items\n",
            "-rw-r--r--   1 root root          9 2023-02-22 18:36 files/jps.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#9\n",
        "!/usr/local/hadoop-3.3.4/bin/hdfs dfs -tail files/jps.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ih4Z8ssAa5g5",
        "outputId": "927785a5-40df-4545-efe0-f57b77d53d79"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8044 Jps\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejercicio 2\n",
        "En el siguiente enlace tenéis un ejemplo de uso de otra de las funciones deiponibles en la distribución Hadoop, en este caso un contador de palabras.\n",
        "\n",
        "[Contar palabras con mapReduce](https://www.youtube.com/watch?v=woUzV_liwto)\n",
        "\n",
        "A partir de lo que has visto en este cuaderno, reproduce el ejempllo en tu entorno, ofreciendo una salida en la que por ejemplo se ordenen las palabras de mayo a menor número de apariciones.\n",
        "\n",
        "[El Quijote texto plano](https://gutenberg.org/files/2000/2000-0.txt)\n",
        "\n",
        "Introduce celdas de texto para explicar los pasos que vas realizando."
      ],
      "metadata": {
        "id": "Dj2ya7U0zxRy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Descargamos el libro"
      ],
      "metadata": {
        "id": "ei1U4Ev4vVuH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://gutenberg.org/files/2000/2000-0.txt /usr/local/hadoop-3.3.4/bin/hdfs/libros"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u1iuUoMkbLM6",
        "outputId": "aba6b4c1-1099-473f-8a8f-48fa919d62c3"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-02-22 19:18:50--  https://gutenberg.org/files/2000/2000-0.txt\n",
            "Resolving gutenberg.org (gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to gutenberg.org (gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2226045 (2.1M) [text/plain]\n",
            "Saving to: ‘2000-0.txt.2’\n",
            "\n",
            "2000-0.txt.2        100%[===================>]   2.12M  1.58MB/s    in 1.3s    \n",
            "\n",
            "2023-02-22 19:18:55 (1.58 MB/s) - ‘2000-0.txt.2’ saved [2226045/2226045]\n",
            "\n",
            "/usr/local/hadoop-3.3.4/bin/hdfs/libros: Scheme missing.\n",
            "FINISHED --2023-02-22 19:18:55--\n",
            "Total wall clock time: 4.4s\n",
            "Downloaded: 1 files, 2.1M in 1.3s (1.58 MB/s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!/usr/local/hadoop-3.3.4/bin/hdfs dfs -head 2000-0.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7VY4dVM7c52t",
        "outputId": "129848df-0397-4cc6-8247-c2c3c4be074e"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "﻿The Project Gutenberg eBook of Don Quijote, by Miguel de Cervantes Saavedra\r\n",
            "\r\n",
            "This eBook is for the use of anyone anywhere in the United States and\r\n",
            "most other parts of the world at no cost and with almost no restrictions\r\n",
            "whatsoever. You may copy it, give it away or re-use it under the terms\r\n",
            "of the Project Gutenberg License included with this eBook or online at\r\n",
            "www.gutenberg.org. If you are not located in the United States, you\r\n",
            "will have to check the laws of the country where you are located before\r\n",
            "using this eBook.\r\n",
            "\r\n",
            "Title: Don Quijote\r\n",
            "\r\n",
            "Author: Miguel de Cervantes Saavedra\r\n",
            "\r\n",
            "Release Date: December, 1999 [eBook #2000]\r\n",
            "[Most recently updated: January 2, 2020]\r\n",
            "\r\n",
            "Language: Spanish\r\n",
            "\r\n",
            "Character set encoding: UTF-8\r\n",
            "\r\n",
            "Produced by: an anonymous Project Gutenberg volunteer and Joaquin Cuenca Abela\r\n",
            "\r\n",
            "*** START OF THE PROJECT GUTENBERG EBOOK DON QUIJOTE ***\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "El ingenioso hidalgo don Quijote de la Mancha\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "por Miguel de Cervantes Saavedra\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "El ingenioso hidalgo don Quijote de la Ma"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creamos la carpeta libros que es donde ubicaremos El Quijote"
      ],
      "metadata": {
        "id": "XXkezIqlv_Aa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!/usr/local/hadoop-3.3.4/bin/hdfs dfs -mkdir libros\n",
        "!ls "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_HUipOVdK35",
        "outputId": "bc98aaf8-3368-43c6-ddd6-e836e8f23ce8"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: `libros': File exists\n",
            "2000-0.txt    2000-0.txt.2  hadoop-3.3.4\t hoy.txt  libros  sample_data\n",
            "2000-0.txt.1  files\t    hadoop-3.3.4.tar.gz  jps.txt  prueba  user.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Movemos el libro a la carpeta de libros en hdfs"
      ],
      "metadata": {
        "id": "Z8gbXS0UwiQT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!/usr/local/hadoop-3.3.4/bin/hdfs dfs -put 2000-0.txt /libros"
      ],
      "metadata": {
        "id": "Gy3_bGjUdu8E"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora pasaremos la aplicación de mapreduce al fichero del libro"
      ],
      "metadata": {
        "id": "dfFAHLZIwpM4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "/usr/local/hadoop-3.3.4/bin/hadoop jar \\\n",
        "  /usr/local/hadoop-3.3.4/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.4.jar \\\n",
        "  wordcount /libros /libros_salida"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XUquoBsqlJad",
        "outputId": "b878db02-13c2-464b-f5e6-f0c9c1791712"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-02-22 19:24:34,755 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2023-02-22 19:24:34,940 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2023-02-22 19:24:34,941 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2023-02-22 19:24:35,217 INFO input.FileInputFormat: Total input files to process : 1\n",
            "2023-02-22 19:24:35,273 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2023-02-22 19:24:35,608 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1814203746_0001\n",
            "2023-02-22 19:24:35,608 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2023-02-22 19:24:35,902 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2023-02-22 19:24:35,903 INFO mapreduce.Job: Running job: job_local1814203746_0001\n",
            "2023-02-22 19:24:35,914 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2023-02-22 19:24:35,926 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2023-02-22 19:24:35,926 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2023-02-22 19:24:35,927 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
            "2023-02-22 19:24:35,997 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2023-02-22 19:24:35,998 INFO mapred.LocalJobRunner: Starting task: attempt_local1814203746_0001_m_000000_0\n",
            "2023-02-22 19:24:36,079 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2023-02-22 19:24:36,082 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2023-02-22 19:24:36,123 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2023-02-22 19:24:36,133 INFO mapred.MapTask: Processing split: file:/libros:0+2226045\n",
            "2023-02-22 19:24:36,342 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2023-02-22 19:24:36,342 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2023-02-22 19:24:36,342 INFO mapred.MapTask: soft limit at 83886080\n",
            "2023-02-22 19:24:36,342 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2023-02-22 19:24:36,342 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2023-02-22 19:24:36,350 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2023-02-22 19:24:36,373 INFO input.LineRecordReader: Found UTF-8 BOM and skipped it\n",
            "2023-02-22 19:24:36,910 INFO mapreduce.Job: Job job_local1814203746_0001 running in uber mode : false\n",
            "2023-02-22 19:24:36,912 INFO mapreduce.Job:  map 0% reduce 0%\n",
            "2023-02-22 19:24:37,125 INFO mapred.LocalJobRunner: \n",
            "2023-02-22 19:24:37,125 INFO mapred.MapTask: Starting flush of map output\n",
            "2023-02-22 19:24:37,125 INFO mapred.MapTask: Spilling map output\n",
            "2023-02-22 19:24:37,125 INFO mapred.MapTask: bufstart = 0; bufend = 3740951; bufvoid = 104857600\n",
            "2023-02-22 19:24:37,125 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 24655524(98622096); length = 1558873/6553600\n",
            "2023-02-22 19:24:38,271 INFO mapred.MapTask: Finished spill 0\n",
            "2023-02-22 19:24:38,308 INFO mapred.Task: Task:attempt_local1814203746_0001_m_000000_0 is done. And is in the process of committing\n",
            "2023-02-22 19:24:38,319 INFO mapred.LocalJobRunner: map\n",
            "2023-02-22 19:24:38,319 INFO mapred.Task: Task 'attempt_local1814203746_0001_m_000000_0' done.\n",
            "2023-02-22 19:24:38,336 INFO mapred.Task: Final Counters for attempt_local1814203746_0001_m_000000_0: Counters: 18\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=2524573\n",
            "\t\tFILE: Number of bytes written=1520981\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=38062\n",
            "\t\tMap output records=389719\n",
            "\t\tMap output bytes=3740951\n",
            "\t\tMap output materialized bytes=602460\n",
            "\t\tInput split bytes=77\n",
            "\t\tCombine input records=389719\n",
            "\t\tCombine output records=39791\n",
            "\t\tSpilled Records=39791\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=15\n",
            "\t\tTotal committed heap usage (bytes)=432013312\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=2243449\n",
            "2023-02-22 19:24:38,337 INFO mapred.LocalJobRunner: Finishing task: attempt_local1814203746_0001_m_000000_0\n",
            "2023-02-22 19:24:38,343 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2023-02-22 19:24:38,347 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2023-02-22 19:24:38,348 INFO mapred.LocalJobRunner: Starting task: attempt_local1814203746_0001_r_000000_0\n",
            "2023-02-22 19:24:38,360 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2023-02-22 19:24:38,360 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2023-02-22 19:24:38,360 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2023-02-22 19:24:38,367 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@6fa2f23d\n",
            "2023-02-22 19:24:38,369 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2023-02-22 19:24:38,398 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2384042240, maxSingleShuffleLimit=596010560, mergeThreshold=1573467904, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2023-02-22 19:24:38,410 INFO reduce.EventFetcher: attempt_local1814203746_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2023-02-22 19:24:38,470 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1814203746_0001_m_000000_0 decomp: 602456 len: 602460 to MEMORY\n",
            "2023-02-22 19:24:38,475 INFO reduce.InMemoryMapOutput: Read 602456 bytes from map-output for attempt_local1814203746_0001_m_000000_0\n",
            "2023-02-22 19:24:38,479 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 602456, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->602456\n",
            "2023-02-22 19:24:38,484 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2023-02-22 19:24:38,486 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2023-02-22 19:24:38,486 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2023-02-22 19:24:38,494 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2023-02-22 19:24:38,494 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 602449 bytes\n",
            "2023-02-22 19:24:38,579 INFO reduce.MergeManagerImpl: Merged 1 segments, 602456 bytes to disk to satisfy reduce memory limit\n",
            "2023-02-22 19:24:38,580 INFO reduce.MergeManagerImpl: Merging 1 files, 602460 bytes from disk\n",
            "2023-02-22 19:24:38,581 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2023-02-22 19:24:38,581 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2023-02-22 19:24:38,582 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 602449 bytes\n",
            "2023-02-22 19:24:38,582 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2023-02-22 19:24:38,586 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2023-02-22 19:24:38,846 INFO mapred.Task: Task:attempt_local1814203746_0001_r_000000_0 is done. And is in the process of committing\n",
            "2023-02-22 19:24:38,847 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2023-02-22 19:24:38,848 INFO mapred.Task: Task attempt_local1814203746_0001_r_000000_0 is allowed to commit now\n",
            "2023-02-22 19:24:38,850 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1814203746_0001_r_000000_0' to file:/libros_salida\n",
            "2023-02-22 19:24:38,852 INFO mapred.LocalJobRunner: reduce > reduce\n",
            "2023-02-22 19:24:38,852 INFO mapred.Task: Task 'attempt_local1814203746_0001_r_000000_0' done.\n",
            "2023-02-22 19:24:38,854 INFO mapred.Task: Final Counters for attempt_local1814203746_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=3729525\n",
            "\t\tFILE: Number of bytes written=2573868\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=39791\n",
            "\t\tReduce shuffle bytes=602460\n",
            "\t\tReduce input records=39791\n",
            "\t\tReduce output records=39791\n",
            "\t\tSpilled Records=39791\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=432013312\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=450427\n",
            "2023-02-22 19:24:38,854 INFO mapred.LocalJobRunner: Finishing task: attempt_local1814203746_0001_r_000000_0\n",
            "2023-02-22 19:24:38,854 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2023-02-22 19:24:38,916 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2023-02-22 19:24:38,917 INFO mapreduce.Job: Job job_local1814203746_0001 completed successfully\n",
            "2023-02-22 19:24:38,935 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=6254098\n",
            "\t\tFILE: Number of bytes written=4094849\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=38062\n",
            "\t\tMap output records=389719\n",
            "\t\tMap output bytes=3740951\n",
            "\t\tMap output materialized bytes=602460\n",
            "\t\tInput split bytes=77\n",
            "\t\tCombine input records=389719\n",
            "\t\tCombine output records=39791\n",
            "\t\tReduce input groups=39791\n",
            "\t\tReduce shuffle bytes=602460\n",
            "\t\tReduce input records=39791\n",
            "\t\tReduce output records=39791\n",
            "\t\tSpilled Records=79582\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=15\n",
            "\t\tTotal committed heap usage (bytes)=864026624\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=2243449\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=450427\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!/usr/local/hadoop-3.3.4/bin/hdfs dfs -ls /"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L57KyjS-et2I",
        "outputId": "ade5de09-aac8-400b-9ede-041d236f0e40"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 29 items\n",
            "-rwxr-xr-x   1 root root          0 2023-02-22 18:10 /.dockerenv\n",
            "-rw-r--r--   1 root root      16047 2022-12-14 20:29 /NGC-DL-CONTAINER-LICENSE\n",
            "drwxr-xr-x   - root root       4096 2023-02-21 14:37 /bin\n",
            "drwxr-xr-x   - root root       4096 2020-04-15 11:09 /boot\n",
            "drwxr-xr-x   - root root       4096 2023-02-22 19:18 /content\n",
            "drwxr-xr-x   - root root       4096 2023-02-21 14:49 /datalab\n",
            "drwxr-xr-x   - root root        360 2023-02-22 18:10 /dev\n",
            "drwxr-xr-x   - root root       4096 2023-02-22 18:10 /etc\n",
            "-rw-r--r--   1 root root          9 2023-02-22 18:31 /files\n",
            "drwxr-xr-x   - root root       4096 2020-04-15 11:09 /home\n",
            "drwxr-xr-x   - root root       4096 2023-02-21 14:31 /lib\n",
            "drwxr-xr-x   - root root       4096 2023-02-21 14:25 /lib32\n",
            "drwxr-xr-x   - root root       4096 2022-11-30 02:06 /lib64\n",
            "-rw-r--r--   1 root root    2226045 2023-02-22 18:51 /libros\n",
            "drwxr-xr-x   - root root       4096 2023-02-22 19:24 /libros_salida\n",
            "drwxr-xr-x   - root root       4096 2022-11-30 02:04 /libx32\n",
            "drwxr-xr-x   - root root       4096 2022-11-30 02:04 /media\n",
            "drwxr-xr-x   - root root       4096 2022-11-30 02:04 /mnt\n",
            "drwxr-xr-x   - root root       4096 2023-02-21 14:50 /opt\n",
            "dr-xr-xr-x   - root root          0 2023-02-22 18:10 /proc\n",
            "drwx------   - root root       4096 2023-02-22 18:14 /root\n",
            "drwxr-xr-x   - root root       4096 2023-02-21 14:32 /run\n",
            "drwxr-xr-x   - root root       4096 2023-02-22 18:10 /sbin\n",
            "drwxr-xr-x   - root root       4096 2022-11-30 02:04 /srv\n",
            "dr-xr-xr-x   - root root          0 2023-02-22 18:10 /sys\n",
            "drwxrwxrwt   - root root       4096 2023-02-22 19:24 /tmp\n",
            "drwxr-xr-x   - root root       4096 2023-02-21 14:49 /tools\n",
            "drwxr-xr-x   - root root       4096 2023-02-21 14:50 /usr\n",
            "drwxr-xr-x   - root root       4096 2023-02-21 14:49 /var\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vemos los dos fichero que nos ha generado la aplicación."
      ],
      "metadata": {
        "id": "b2LmVcY4w1VW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!/usr/local/hadoop-3.3.4/bin/hdfs dfs -ls /libros_salida/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKYCcYUHg5E4",
        "outputId": "d8eaf398-de35-4cbd-c1b2-9d1e7edd419c"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2 items\n",
            "-rw-r--r--   1 root root          0 2023-02-22 19:24 /libros_salida/_SUCCESS\n",
            "-rw-r--r--   1 root root     446927 2023-02-22 19:24 /libros_salida/part-r-00000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Descargamos desde el hdfs al local para poder trabajar con el."
      ],
      "metadata": {
        "id": "WP-Waq4wxC91"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!/usr/local/hadoop-3.3.4/bin/hdfs dfs -get /libros_salida/part-r-00000"
      ],
      "metadata": {
        "id": "-TmBgWaImm3_"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora vamos a procesar el fichero para que nos aparezca las palabras mas usadas del libro."
      ],
      "metadata": {
        "id": "sATGc6A8xYXk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        " \n",
        "!cat part-r-00000 | sort -t$'\\t' -k2 -r -n | head -n 100 \n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFc3_Wcgn6Ps",
        "outputId": "3d7e08d7-c8c1-4259-cbd3-9a02c98873c0"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "que\t19546\n",
            "de\t18134\n",
            "y\t15976\n",
            "la\t10329\n",
            "a\t9627\n",
            "el\t8009\n",
            "en\t7941\n",
            "no\t5622\n",
            "se\t4751\n",
            "los\t4701\n",
            "con\t4119\n",
            "por\t3763\n",
            "las\t3440\n",
            "lo\t3418\n",
            "le\t3405\n",
            "su\t3355\n",
            "—\t2983\n",
            "don\t2597\n",
            "del\t2501\n",
            "me\t2344\n",
            "como\t2232\n",
            "es\t1993\n",
            "un\t1932\n",
            "más\t1834\n",
            "si\t1780\n",
            "al\t1711\n",
            "yo\t1703\n",
            "mi\t1684\n",
            "para\t1421\n",
            "ni\t1351\n",
            "una\t1305\n",
            "y,\t1250\n",
            "tan\t1219\n",
            "porque\t1189\n",
            "o\t1163\n",
            "sin\t1141\n",
            "que,\t1071\n",
            "sus\t1049\n",
            "ha\t1038\n",
            "él\t1038\n",
            "había\t1009\n",
            "ser\t1000\n",
            "Sancho\t985\n",
            "todo\t964\n",
            "Quijote\t954\n",
            "—dijo\t870\n",
            "bien\t863\n",
            "—respondió\t803\n",
            "vuestra\t792\n",
            "señor\t734\n",
            "te\t724\n",
            "todos\t704\n",
            "era\t703\n",
            "ya\t689\n",
            "sino\t687\n",
            "merced\t678\n",
            "Y\t670\n",
            "cuando\t666\n",
            "dos\t634\n",
            "donde\t617\n",
            "fue\t614\n",
            "este\t609\n",
            "quien\t608\n",
            "esta\t592\n",
            "pero\t577\n",
            "qué\t551\n",
            "Quijote,\t536\n",
            "cual\t534\n",
            "muy\t530\n",
            "he\t529\n",
            "aunque\t511\n",
            "esto\t507\n",
            "Sancho,\t486\n",
            "así\t483\n",
            "aquel\t478\n",
            "hacer\t474\n",
            "os\t464\n",
            "decir\t459\n",
            "otra\t458\n",
            "dijo:\t457\n",
            "son\t453\n",
            "sobre\t451\n",
            "nos\t450\n",
            "buen\t443\n",
            "hay\t441\n",
            "ella\t419\n",
            "está\t416\n",
            "pues\t410\n",
            "No\t410\n",
            "mal\t410\n",
            "mí\t405\n",
            "dijo\t405\n",
            "así,\t405\n",
            "otro\t404\n",
            "aquí\t395\n",
            "caballero\t390\n",
            "mis\t385\n",
            "cosa\t380\n",
            "hasta\t377\n",
            "ver\t375\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora guardaremos el resultado en un archivo que nos descargamos."
      ],
      "metadata": {
        "id": "gb4ZEbYpxpZa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat part-r-00000 | sort -t$'\\t' -k2 -r -n | head -n 100 > resultado.txt\n",
        "files.download('resultado.txt')"
      ],
      "metadata": {
        "id": "WJy3GeKixd4S"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}