{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk import tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import cess_esp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\alvaro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package cess_esp to\n",
      "[nltk_data]     C:\\Users\\alvaro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package cess_esp is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Descargar recursos necesarios\n",
    "nltk.download('stopwords')\n",
    "nltk.download('cess_esp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta_archivo = \"../datasets/ejemplo1.txt\"\n",
    "with open(ruta_archivo, 'r', encoding='utf-8') as archivo:\n",
    "    texto = archivo.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Son', '3', 'muertes', 'conocidas', 'día', 'hoy']\n",
      "['Hacer', 'balance', 'pandemia', 'expertos', 'atreven', 'todavía', 'hacer', 'registros', 'diarios', 'siguen', 'arrojando', 'fallecidos', 'virus', 'seguirá', 'tiempo', 'aunque', 'provoque', 'menos', 'daños', '.', 'España', 'atraviesa', 'semanas', 'final', 'fase', 'epidémica', '.']\n",
      "['Se', 'confirmado', 'mejor', 'registro', 'hospitalizados', 'serie', 'histórica', ':', 'nunca', 'menos', '2.100', 'personas', 'ingresadas', 'infección', 'registra', 'capacidad', 'asistencial', '.']\n",
      "[('Son', 'vsip3p0'), ('3', 'Z'), ('muertes', 'ncfp000'), ('conocidas', 'aq0fpp'), ('día', 'ncms000'), ('hoy', 'rg')]\n",
      "[('Hacer', 'vmn0000'), ('balance', 'ncms000'), ('pandemia', None), ('expertos', 'ncmp000'), ('atreven', 'vmip3p0'), ('todavía', 'rg'), ('hacer', 'vmn0000'), ('registros', 'ncmp000'), ('diarios', 'ncmp000'), ('siguen', 'vmip3p0'), ('arrojando', 'vmg0000'), ('fallecidos', 'aq0mpp'), ('virus', 'ncmn000'), ('seguirá', 'vmif3s0'), ('tiempo', 'ncms000'), ('aunque', 'cs'), ('provoque', None), ('menos', 'rg'), ('daños', 'ncmp000'), ('.', 'Fp'), ('España', 'np0000l'), ('atraviesa', 'vmip3s0'), ('semanas', 'ncfp000'), ('final', 'aq0cs0'), ('fase', 'ncfs000'), ('epidémica', None), ('.', 'Fp')]\n",
      "[('Se', 'p0000000'), ('confirmado', None), ('mejor', 'aq0cs0'), ('registro', 'ncms000'), ('hospitalizados', 'aq0mpp'), ('serie', 'ncfs000'), ('histórica', 'aq0fs0'), (':', 'Fd'), ('nunca', 'rg'), ('menos', 'rg'), ('2.100', None), ('personas', 'ncfp000'), ('ingresadas', None), ('infección', 'ncfs000'), ('registra', 'vmip3s0'), ('capacidad', 'ncfs000'), ('asistencial', None), ('.', 'Fp')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "title_start = texto.find('Titulo')\n",
    "news_start = texto.find('Noticia')\n",
    "summary_start = texto.find('Resumen')\n",
    "    \n",
    "# Extract the contents of each section\n",
    "titulo = texto[title_start + 6:news_start]\n",
    "noticia = texto[news_start + 7:summary_start]\n",
    "resumen = texto[summary_start + 7:]\n",
    "\n",
    "# Extraer tokens y convertir a minúsculas\n",
    "#Si se usa el corpus de Cess_esp es necesario que las palabras se encuentren en mayusculas\n",
    "# tokens_titulo = [token.lower() for token in word_tokenize(titulo)]\n",
    "# tokens_noticia = [token.lower() for token in word_tokenize(noticia)]\n",
    "# tokens_resumen = [token.lower() for token in word_tokenize(resumen)]\n",
    "\n",
    "tokens_titulo = [token for token in word_tokenize(titulo)]\n",
    "tokens_noticia = [token for token in word_tokenize(noticia)]\n",
    "tokens_resumen = [token for token in word_tokenize(resumen)]\n",
    "\n",
    "# Eliminar acentos\n",
    "tabla_conversion = str.maketrans('áéíóúü', 'aeiouu')\n",
    "#Tambien es necesario que las palabras tengan sus respectivos acentos.\n",
    "# tokens_titulo = [token.translate(tabla_conversion) for token in tokens_titulo]\n",
    "# tokens_noticia = [token.translate(tabla_conversion) for token in tokens_noticia]\n",
    "# tokens_resumen = [token.translate(tabla_conversion) for token in tokens_resumen]\n",
    "\n",
    "# Eliminar stopwords\n",
    "stop_words = set(stopwords.words('spanish'))\n",
    "tokens_titulo = [token for token in tokens_titulo if not token in stop_words]\n",
    "tokens_noticia = [token for token in tokens_noticia if not token in stop_words]\n",
    "tokens_resumen = [token for token in tokens_resumen if not token in stop_words]\n",
    "\n",
    "\n",
    "print(tokens_titulo)\n",
    "print(tokens_noticia)\n",
    "print(tokens_resumen)\n",
    "\n",
    "\n",
    "# Extraer POS tags y eliminar palabras que no sean nombres comunes\n",
    "\n",
    "\n",
    "sustantivos=[]\n",
    "tagger=tag.UnigramTagger(cess_esp.tagged_sents())\n",
    "\n",
    "tokens_titulo=tagger.tag(tokens_titulo)\n",
    "tokens_noticia=tagger.tag(tokens_noticia)\n",
    "tokens_resumen=tagger.tag(tokens_resumen)\n",
    "\n",
    "print(tokens_titulo)\n",
    "print(tokens_noticia)\n",
    "print(tokens_resumen)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sustantivos en el Título: ['muertes', 'día'] \n",
      "\n",
      "Sustantivos en la Noticia: ['balance', 'expertos', 'registros', 'diarios', 'virus', 'tiempo', 'daños', 'España', 'semanas', 'fase'] \n",
      "\n",
      "Sustantivos en el Resumen: ['registro', 'serie', 'personas', 'infección', 'capacidad'] \n",
      "\n",
      "Fallos que el corpus no ha sido capaz de resolver:  \n",
      " ['Son', '3', 'conocidas', 'hoy', 'Hacer', 'pandemia', 'atreven', 'todavía', 'hacer', 'siguen', 'arrojando', 'fallecidos', 'seguirá', 'aunque', 'provoque', 'menos', '.', 'atraviesa', 'final', 'epidémica', '.', 'Se', 'confirmado', 'mejor', 'hospitalizados', 'histórica', ':', 'nunca', 'menos', '2.100', 'ingresadas', 'registra', 'asistencial', '.']\n"
     ]
    }
   ],
   "source": [
    "fallo = []\n",
    "def find_nouns(tagged_words):\n",
    "    \"\"\"\n",
    "    Encuentra las palabras que tienen la etiqueta \"NN\" (sustantivo) en una lista de tuplas que representan palabras etiquetadas con su parte del habla (PoS).\n",
    "    \n",
    "    :param tagged_words: La lista de tuplas que representan palabras etiquetadas con su PoS.\n",
    "    :return: Una lista de las palabras que tienen la etiqueta \"NN\".\n",
    "    \"\"\"\n",
    "    nouns = []\n",
    "    \n",
    "    for word, tag in tagged_words:\n",
    "        tag=str(tag)\n",
    "        if re.search(\"^n\", tag):\n",
    "            nouns.append(word)\n",
    "        else:\n",
    "            fallo.append(word)\n",
    "    return nouns\n",
    "    \n",
    "\n",
    "sustantivos_titulo=find_nouns(tokens_titulo)\n",
    "sustantivos_noticia=find_nouns(tokens_noticia)\n",
    "sustantivos_resumen=find_nouns(tokens_resumen)\n",
    "\n",
    "# Imprimir resultados\n",
    "print(\"Sustantivos en el Título:\", sustantivos_titulo,\"\\n\")\n",
    "print(\"Sustantivos en la Noticia:\", sustantivos_noticia,\"\\n\")\n",
    "print(\"Sustantivos en el Resumen:\", sustantivos_resumen,\"\\n\")\n",
    "\n",
    "print(\"Fallos que el corpus no ha sido capaz de resolver: \",\"\\n\",fallo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Guardamos los sustantivos en un fichero:\n",
    "with open('sustantivos.txt', 'w') as file:\n",
    "    file.write(str(sustantivos_titulo))\n",
    "    file.write(str(sustantivos_noticia))\n",
    "    file.write(str(sustantivos_resumen))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
