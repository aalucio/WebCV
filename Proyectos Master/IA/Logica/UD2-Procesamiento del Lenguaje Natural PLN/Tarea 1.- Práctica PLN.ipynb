{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk import tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import cess_esp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\alvaro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package cess_esp to\n",
      "[nltk_data]     C:\\Users\\alvaro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package cess_esp is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Descargar recursos necesarios\n",
    "nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('cess_esp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta_archivo = \"../datasets/ejemplo1.txt\"\n",
    "with open(ruta_archivo, 'r', encoding='utf-8') as archivo:\n",
    "    texto = archivo.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Son 3 las muertes conocidas en el día de hoy\n",
      "\n",
      " \n",
      "\n",
      "Hacer balance de una pandemia es algo que ni los expertos se atreven todavía a \n",
      "hacer cuando los registros diarios siguen arrojando fallecidos por un virus que seguirá \n",
      "mucho tiempo entre nosotros aunque provoque menos daños. España atraviesa en estas semanas \n",
      "el final de la fase epidémica. \n",
      "\n",
      " \n",
      "\n",
      "Se ha confirmado el mejor registro de hospitalizados de la serie histórica: nunca antes \n",
      "habíamos tenido menos de 2.100 personas ingresadas por la infección desde que se registra la capacidad asistencial.\n",
      "\n",
      "\n",
      "\n",
      "['3', 'muertes', 'conocidas', 'dia', 'hoy']\n",
      "['hacer', 'balance', 'pandemia', 'expertos', 'atreven', 'todavia', 'hacer', 'registros', 'diarios', 'siguen', 'arrojando', 'fallecidos', 'virus', 'seguira', 'tiempo', 'aunque', 'provoque', 'menos', 'daños', '.', 'españa', 'atraviesa', 'semanas', 'final', 'fase', 'epidemica', '.']\n",
      "['confirmado', 'mejor', 'registro', 'hospitalizados', 'serie', 'historica', ':', 'nunca', 'habiamos', 'menos', '2.100', 'personas', 'ingresadas', 'infeccion', 'registra', 'capacidad', 'asistencial', '.']\n",
      "[[('El', 'da0ms0'), ('grupo', 'ncms000'), ('estatal', 'aq0cs0'), ('Electricité_de_France', 'np00000'), ('-Fpa-', 'Fpa'), ('EDF', 'np00000'), ('-Fpt-', 'Fpt'), ('anunció', 'vmis3s0'), ('hoy', 'rg'), (',', 'Fc'), ('jueves', 'W'), (',', 'Fc'), ('la', 'da0fs0'), ('compra', 'ncfs000'), ('del', 'spcms'), ('51_por_ciento', 'Zp'), ('de', 'sps00'), ('la', 'da0fs0'), ('empresa', 'ncfs000'), ('mexicana', 'aq0fs0'), ('Electricidad_Águila_de_Altamira', 'np00000'), ('-Fpa-', 'Fpa'), ('EAA', 'np00000'), ('-Fpt-', 'Fpt'), (',', 'Fc'), ('creada', 'aq0fsp'), ('por', 'sps00'), ('el', 'da0ms0'), ('japonés', 'aq0ms0'), ('Mitsubishi_Corporation', 'np00000'), ('para', 'sps00'), ('poner_en_marcha', 'vmn0000'), ('una', 'di0fs0'), ('central', 'ncfs000'), ('de', 'sps00'), ('gas', 'ncms000'), ('de', 'sps00'), ('495', 'Z'), ('megavatios', 'ncmp000'), ('.', 'Fp')], [('Una', 'di0fs0'), ('portavoz', 'nccs000'), ('de', 'sps00'), ('EDF', 'np00000'), ('explicó', 'vmis3s0'), ('a', 'sps00'), ('EFE', 'np00000'), ('que', 'cs'), ('el', 'da0ms0'), ('proyecto', 'ncms000'), ('para', 'sps00'), ('la', 'da0fs0'), ('construcción', 'ncfs000'), ('de', 'sps00'), ('Altamira_2', 'np00000'), (',', 'Fc'), ('al', 'spcms'), ('norte', 'ncms000'), ('de', 'sps00'), ('Tampico', 'np00000'), (',', 'Fc'), ('prevé', 'vmm02s0'), ('la', 'da0fs0'), ('utilización', 'ncfs000'), ('de', 'sps00'), ('gas', 'ncms000'), ('natural', 'aq0cs0'), ('como', 'cs'), ('combustible', 'ncms000'), ('principal', 'aq0cs0'), ('en', 'sps00'), ('una', 'di0fs0'), ('central', 'ncfs000'), ('de', 'sps00'), ('ciclo', 'ncms000'), ('combinado', 'aq0msp'), ('que', 'pr0cn000'), ('debe', 'vmip3s0'), ('empezar', 'vmn0000'), ('a', 'sps00'), ('funcionar', 'vmn0000'), ('en', 'sps00'), ('mayo_del_2002', 'W'), ('.', 'Fp')], ...]\n",
      "<UnigramTagger: size=25464>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'UnigramTagger' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 43\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[39m# pos_noticia =tag.UnigramTagger(pos_noticia)\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[39m# pos_resumen =tag.UnigramTagger(pos_resumen)\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[39mprint\u001b[39m (pos_titulo)\n\u001b[1;32m---> 43\u001b[0m sustantivos_titulo \u001b[39m=\u001b[39m [word \u001b[39mfor\u001b[39;00m word, pos \u001b[39min\u001b[39;00m pos_titulo \u001b[39mif\u001b[39;00m pos\u001b[39m.\u001b[39mstartswith(\u001b[39m'\u001b[39m\u001b[39mnc\u001b[39m\u001b[39m'\u001b[39m)]\n\u001b[0;32m     44\u001b[0m \u001b[39m# sustantivos_noticia = [word for word, pos in pos_noticia if pos.startswith('nc')]\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[39m# sustantivos_resumen = [word for word, pos in pos_resumen if pos.startswith('nc')]\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \n\u001b[0;32m     47\u001b[0m \u001b[39m# Imprimir resultados\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mSustantivos en el Título:\u001b[39m\u001b[39m\"\u001b[39m, sustantivos_titulo)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'UnigramTagger' object is not iterable"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "title_start = texto.find('Titulo')\n",
    "news_start = texto.find('Noticia')\n",
    "summary_start = texto.find('Resumen')\n",
    "    \n",
    "# Extract the contents of each section\n",
    "titulo = texto[title_start + 6:news_start]\n",
    "noticia = texto[news_start + 7:summary_start]\n",
    "resumen = texto[summary_start + 7:]\n",
    "print(titulo,noticia,resumen)\n",
    "# Extraer tokens y convertir a minúsculas\n",
    "tokens_titulo = [token.lower() for token in word_tokenize(titulo)]\n",
    "tokens_noticia = [token.lower() for token in word_tokenize(noticia)]\n",
    "tokens_resumen = [token.lower() for token in word_tokenize(resumen)]\n",
    "\n",
    "# Eliminar acentos\n",
    "tabla_conversion = str.maketrans('áéíóúü', 'aeiouu')\n",
    "tokens_titulo = [token.translate(tabla_conversion) for token in tokens_titulo]\n",
    "tokens_noticia = [token.translate(tabla_conversion) for token in tokens_noticia]\n",
    "tokens_resumen = [token.translate(tabla_conversion) for token in tokens_resumen]\n",
    "\n",
    "# Eliminar stopwords\n",
    "stop_words = set(stopwords.words('spanish'))\n",
    "tokens_titulo = [token for token in tokens_titulo if not token in stop_words]\n",
    "tokens_noticia = [token for token in tokens_noticia if not token in stop_words]\n",
    "tokens_resumen = [token for token in tokens_resumen if not token in stop_words]\n",
    "\n",
    "\n",
    "print(tokens_titulo)\n",
    "print(tokens_noticia)\n",
    "print(tokens_resumen)\n",
    "\n",
    "# Extraer POS tags y eliminar palabras que no sean nombres comunes\n",
    "# pos_titulo = cess_esp.tagged_sents(tokens_titulo)\n",
    "tokens_titulo=cess_esp.tagged_sents()\n",
    "print(tokens_titulo)\n",
    "# pos_noticia = cess_esp.tagged_sents(tokens_noticia)\n",
    "# pos_resumen = cess_esp.tagged_sents(tokens_resumen)\n",
    "\n",
    "pos_titulo =tag.UnigramTagger(tokens_titulo)\n",
    "# pos_noticia =tag.UnigramTagger(pos_noticia)\n",
    "# pos_resumen =tag.UnigramTagger(pos_resumen)\n",
    "print (pos_titulo)\n",
    "sustantivos_titulo = [word for word, pos in pos_titulo if pos.startswith('nc')]\n",
    "# sustantivos_noticia = [word for word, pos in pos_noticia if pos.startswith('nc')]\n",
    "# sustantivos_resumen = [word for word, pos in pos_resumen if pos.startswith('nc')]\n",
    "\n",
    "# Imprimir resultados\n",
    "print(\"Sustantivos en el Título:\", sustantivos_titulo)\n",
    "# print(\"Sustantivos en la Noticia:\", sustantivos_noticia)\n",
    "# print(\"Sustantivos en el Resumen:\", sustantivos_resumen)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
